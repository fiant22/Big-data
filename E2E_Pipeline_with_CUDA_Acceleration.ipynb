{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: numpy in c:\\bintang\\lib\\site-packages (2.3.2)\n",
            "Requirement already satisfied: pandas in c:\\bintang\\lib\\site-packages (2.3.2)\n",
            "Requirement already satisfied: scikit-learn in c:\\bintang\\lib\\site-packages (1.7.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\lenovo\\appdata\\roaming\\python\\python313\\site-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in c:\\bintang\\lib\\site-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in c:\\bintang\\lib\\site-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: scipy>=1.8.0 in c:\\bintang\\lib\\site-packages (from scikit-learn) (1.16.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in c:\\bintang\\lib\\site-packages (from scikit-learn) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\bintang\\lib\\site-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: six>=1.5 in c:\\users\\lenovo\\appdata\\roaming\\python\\python313\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 24.3.1 -> 25.2\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        }
      ],
      "source": [
        "!pip install numpy pandas scikit-learn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "74GpI1yKT-c0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DATA_DIR: Data\n",
            "BASE_DIR: .\n",
            "LABEL_COL: label\n",
            "TIME_SERIES: False\n",
            "USE_CUDA: True\n",
            "⚠️ Peringatan: Pustaka RAPIDS tidak ditemukan. Mengubah USE_CUDA menjadi False.\n",
            "cv2 available: False\n",
            "pytesseract available: False\n",
            "Artifact directories ensured:\n",
            " - output: .\\output\n",
            " - extracted: .\\output\\extracted\n",
            " - models: .\\models\n",
            " - reports: .\\reports\n",
            " - predictions: .\\predictions\n",
            " - logs: .\\logs\n",
            " - config: .\\config\n",
            "Templates & config written: .\\README_TEMPLATE.md .\\reports\\evaluation_report.txt .\\config\\indicator_configs.json\n",
            "Inventory written: .\\output\\inventory.csv\n",
            "  indicator       type                    path\n",
            "0       C01  train_csv  Data\\C01\\train C01.csv\n",
            "1       C01   test_csv   Data\\C01\\test C01.csv\n",
            "2       C02  train_csv  Data\\C02\\train C02.csv\n",
            "3       C02   test_csv   Data\\C02\\test C02.csv\n",
            "4       C03  train_csv  Data\\C03\\train C03.csv\n",
            "Extraction complete.\n",
            "Master extracted written: .\\output\\extracted_master.csv\n",
            "Master empty, skipping imputation.\n",
            "Running Feature Engineering on CPU with Pandas...\n"
          ]
        },
        {
          "ename": "KeyError",
          "evalue": "'date'",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Bintang\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3812\u001b[39m, in \u001b[36mIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   3811\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3812\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3813\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
            "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/index.pyx:167\u001b[39m, in \u001b[36mpandas._libs.index.IndexEngine.get_loc\u001b[39m\u001b[34m()\u001b[39m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/index.pyx:196\u001b[39m, in \u001b[36mpandas._libs.index.IndexEngine.get_loc\u001b[39m\u001b[34m()\u001b[39m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/hashtable_class_helper.pxi:7088\u001b[39m, in \u001b[36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[39m\u001b[34m()\u001b[39m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/hashtable_class_helper.pxi:7096\u001b[39m, in \u001b[36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[39m\u001b[34m()\u001b[39m\n",
            "\u001b[31mKeyError\u001b[39m: 'date'",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 290\u001b[39m\n\u001b[32m    288\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mRunning Feature Engineering on CPU with Pandas...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    289\u001b[39m df = pd.read_csv(imputed_path)\n\u001b[32m--> \u001b[39m\u001b[32m290\u001b[39m df[\u001b[33m'\u001b[39m\u001b[33mdate\u001b[39m\u001b[33m'\u001b[39m] = pd.to_datetime(\u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mdate\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m, errors=\u001b[33m'\u001b[39m\u001b[33mcoerce\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m    291\u001b[39m df = df.dropna(subset=[\u001b[33m'\u001b[39m\u001b[33mdate\u001b[39m\u001b[33m'\u001b[39m]).sort_values(\u001b[33m'\u001b[39m\u001b[33mdate\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m    293\u001b[39m feat = df.copy()\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Bintang\\Lib\\site-packages\\pandas\\core\\frame.py:4107\u001b[39m, in \u001b[36mDataFrame.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   4105\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.columns.nlevels > \u001b[32m1\u001b[39m:\n\u001b[32m   4106\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._getitem_multilevel(key)\n\u001b[32m-> \u001b[39m\u001b[32m4107\u001b[39m indexer = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4108\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[32m   4109\u001b[39m     indexer = [indexer]\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Bintang\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3819\u001b[39m, in \u001b[36mIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   3814\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[32m   3815\u001b[39m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc.Iterable)\n\u001b[32m   3816\u001b[39m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[32m   3817\u001b[39m     ):\n\u001b[32m   3818\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[32m-> \u001b[39m\u001b[32m3819\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01merr\u001b[39;00m\n\u001b[32m   3820\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[32m   3821\u001b[39m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[32m   3822\u001b[39m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[32m   3823\u001b[39m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[32m   3824\u001b[39m     \u001b[38;5;28mself\u001b[39m._check_indexing_error(key)\n",
            "\u001b[31mKeyError\u001b[39m: 'date'"
          ]
        }
      ],
      "source": [
        "\n",
        "# # E2E Dataset Pipeline — Notebook (dengan Akselerasi CUDA)\n",
        "#\n",
        "# Versi notebook yang sama seperti sebelumnya namun **tidak menggunakan `def` atau function**.\n",
        "# Semua langkah ditulis sebagai blok kode top-level yang bisa dijalankan berurutan.\n",
        "#\n",
        "# ## Perubahan\n",
        "# - Ditambahkan flag `USE_CUDA` untuk beralih antara eksekusi CPU (Scikit-learn) dan GPU (RAPIDS cuDF & cuML).\n",
        "# - Pustaka RAPIDS akan digunakan untuk mempercepat pemrosesan data dan training model jika `USE_CUDA = True` dan environment-nya mendukung.\n",
        "# - Pastikan Anda telah menginstal RAPIDS. Cara termudah adalah menggunakan Conda:\n",
        "# ```bash\n",
        "# # conda create -n rapids -c rapidsai -c conda-forge -c nvidia rapids=23.10 python=3.10 cudatoolkit=11.8\n",
        "# # conda activate rapids\n",
        "# ```\n",
        "\n",
        "# %%\n",
        "# 1) Konfigurasi — sesuaikan path sebelum menjalankan sel-sel berikut\n",
        "DATA_DIR = \"Data\"      # folder berisi C01..C10\n",
        "BASE_DIR = \".\"         # tempat output/artifact akan ditulis (output/, models/, reports/, ...)\n",
        "LABEL_COL = \"label\"    # nama kolom label di file train*.csv\n",
        "TIME_SERIES = False    # True jika dataset adalah time-series\n",
        "\n",
        "# --- TAMBAHAN: Konfigurasi CUDA ---\n",
        "# Ubah menjadi True jika Anda ingin menggunakan akselerasi GPU dengan RAPIDS\n",
        "USE_CUDA = True\n",
        "\n",
        "print(\"DATA_DIR:\", DATA_DIR)\n",
        "print(\"BASE_DIR:\", BASE_DIR)\n",
        "print(\"LABEL_COL:\", LABEL_COL)\n",
        "print(\"TIME_SERIES:\", TIME_SERIES)\n",
        "print(f\"USE_CUDA: {USE_CUDA}\")\n",
        "\n",
        "# %%\n",
        "# 2) Imports & dependensi (dengan tambahan untuk CUDA)\n",
        "from __future__ import annotations\n",
        "import os\n",
        "import re\n",
        "import json\n",
        "import glob\n",
        "import textwrap\n",
        "import math\n",
        "\n",
        "# --- Pustaka CPU (Pandas, Scikit-learn) ---\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from dateutil import parser as dateparser\n",
        "from sklearn.model_selection import StratifiedKFold, TimeSeriesSplit, train_test_split\n",
        "from sklearn.metrics import f1_score, classification_report, confusion_matrix\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.experimental import enable_iterative_imputer  # noqa: F401\n",
        "from sklearn.impute import IterativeImputer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from joblib import dump, load\n",
        "import pickle # Diperlukan untuk menyimpan model cuML\n",
        "\n",
        "# --- TAMBAHAN: Import pustaka GPU (RAPIDS) jika USE_CUDA aktif ---\n",
        "if USE_CUDA:\n",
        "    try:\n",
        "        import cudf\n",
        "        import cupy as cp\n",
        "        from cuml.model_selection import train_test_split as cuml_train_test_split\n",
        "        from cuml.metrics import f1_score as cuml_f1_score, confusion_matrix as cuml_confusion_matrix\n",
        "        from cuml.preprocessing import StandardScaler as cuml_StandardScaler\n",
        "        from cuml.impute import SimpleImputer as cuml_SimpleImputer\n",
        "        from cuml.linear_model import LogisticRegression as cuml_LogisticRegression\n",
        "        from cuml.ensemble import RandomForestClassifier as cuml_RandomForestClassifier\n",
        "        print(\"✅ Pustaka RAPIDS (cuDF, cuML) berhasil diimpor.\")\n",
        "    except ImportError:\n",
        "        print(\"⚠️ Peringatan: Pustaka RAPIDS tidak ditemukan. Mengubah USE_CUDA menjadi False.\")\n",
        "        USE_CUDA = False\n",
        "else:\n",
        "    print(\"Eksekusi menggunakan CPU (Pandas, Scikit-learn).\")\n",
        "\n",
        "# Optional imports for extraction\n",
        "try:\n",
        "    import cv2\n",
        "except Exception:\n",
        "    cv2 = None\n",
        "\n",
        "try:\n",
        "    import pytesseract\n",
        "    from PIL import Image\n",
        "except Exception:\n",
        "    pytesseract = None\n",
        "    Image = None\n",
        "\n",
        "LABEL_MAP = {\"Low\": 0, \"Medium\": 1, \"High\": 2, 0: 0, 1: 1, 2: 2}\n",
        "\n",
        "print(\"cv2 available:\", cv2 is not None)\n",
        "print(\"pytesseract available:\", pytesseract is not None)\n",
        "\n",
        "\n",
        "# %%\n",
        "# 3) Siapkan direktori artifact (tanpa fungsi)\n",
        "paths = {\n",
        "    \"output\": os.path.join(BASE_DIR, \"output\"),\n",
        "    \"extracted\": os.path.join(BASE_DIR, \"output\", \"extracted\"),\n",
        "    \"models\": os.path.join(BASE_DIR, \"models\"),\n",
        "    \"reports\": os.path.join(BASE_DIR, \"reports\"),\n",
        "    \"predictions\": os.path.join(BASE_DIR, \"predictions\"),\n",
        "    \"logs\": os.path.join(BASE_DIR, \"logs\"),\n",
        "    \"config\": os.path.join(BASE_DIR, \"config\"),\n",
        "}\n",
        "for p in paths.values():\n",
        "    os.makedirs(p, exist_ok=True)\n",
        "\n",
        "print(\"Artifact directories ensured:\")\n",
        "for k, v in paths.items():\n",
        "    print(f\" - {k}: {v}\")\n",
        "\n",
        "# %%\n",
        "# 4) Tulis templates (README, contoh config, evaluation report)\n",
        "# (Tidak ada perubahan di sel ini)\n",
        "readme = textwrap.dedent(\n",
        "    \"\"\"\n",
        "    # Project README (Template)\n",
        "\n",
        "    ## Tujuan\n",
        "    - Menghasilkan `predictions/test_predictions.csv` dengan format label numerik (Low=0, Medium=1, High=2).\n",
        "    - Artefak wajib:\n",
        "      - `output/extracted_master.csv`\n",
        "      - `output/master_imputed.csv`\n",
        "      - `models/best_model.joblib` atau `models/best_model.pkl`\n",
        "      - `reports/evaluation_report.txt`\n",
        "      - `logs/failed_images.csv`\n",
        "    \"\"\"\n",
        ").strip()\n",
        "\n",
        "readme_path = os.path.join(BASE_DIR, \"README_TEMPLATE.md\")\n",
        "with open(readme_path, \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(readme)\n",
        "\n",
        "eval_template = textwrap.dedent(\n",
        "    \"\"\"\n",
        "    EVALUATION REPORT (Auto-generated)\n",
        "    =================================\n",
        "    [Macro F1]\n",
        "    {macro_f1}\n",
        "    [Per-class Metrics]\n",
        "    {classification_report}\n",
        "    [Confusion Matrix]\n",
        "    {confusion_matrix}\n",
        "    \"\"\"\n",
        ").strip()\n",
        "eval_path = os.path.join(paths[\"reports\"], \"evaluation_report.txt\")\n",
        "with open(eval_path, \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(eval_template)\n",
        "\n",
        "cfg = { \"indicator_configs\": { \"C01\": { \"has_legend\": True } } }\n",
        "cfg_path = os.path.join(paths[\"config\"], \"indicator_configs.json\")\n",
        "with open(cfg_path, \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(cfg, f, indent=2)\n",
        "\n",
        "print(\"Templates & config written:\", readme_path, eval_path, cfg_path)\n",
        "\n",
        "\n",
        "# %%\n",
        "# 5) Inventory (scan Data/C* untuk gambar & csv)\n",
        "# (Tidak ada perubahan di sel ini)\n",
        "rows = []\n",
        "for cdir in sorted(glob.glob(os.path.join(DATA_DIR, \"C*\"))):\n",
        "    indicator = os.path.basename(cdir)\n",
        "    imgs = glob.glob(os.path.join(cdir, \"*.png\")) + glob.glob(os.path.join(cdir, \"*.jpg\"))\n",
        "    train_csvs = glob.glob(os.path.join(cdir, \"train*.csv\"))\n",
        "    test_csvs = glob.glob(os.path.join(cdir, \"test*.csv\"))\n",
        "    for p in imgs:\n",
        "        rows.append({\"indicator\": indicator, \"type\": \"image\", \"path\": p})\n",
        "    for p in train_csvs:\n",
        "        rows.append({\"indicator\": indicator, \"type\": \"train_csv\", \"path\": p})\n",
        "    for p in test_csvs:\n",
        "        rows.append({\"indicator\": indicator, \"type\": \"test_csv\", \"path\": p})\n",
        "\n",
        "inv = pd.DataFrame(rows)\n",
        "inv_path = os.path.join(paths[\"output\"], \"inventory.csv\")\n",
        "inv.to_csv(inv_path, index=False)\n",
        "print(\"Inventory written:\", inv_path)\n",
        "print(inv.head())\n",
        "\n",
        "# %%\n",
        "# 6) Extraction (inline, best-effort).\n",
        "# (Tidak ada perubahan di sel ini, karena ekstraksi gambar tetap di CPU)\n",
        "failed_rows = []\n",
        "extracted_files = []\n",
        "for cdir in sorted(glob.glob(os.path.join(DATA_DIR, \"C*\"))):\n",
        "    indicator = os.path.basename(cdir)\n",
        "    imgs = sorted(glob.glob(os.path.join(cdir, \"*.png\")) + glob.glob(os.path.join(cdir, \"*.jpg\")))\n",
        "    all_rows = []\n",
        "    for img_path in imgs:\n",
        "        m = re.search(r\"(19|20)\\d{2}\", os.path.basename(img_path))\n",
        "        year = int(m.group(0)) if m else None\n",
        "        date_iso = f\"{year:04d}-01-01\" if year else None\n",
        "\n",
        "        # Placeholder extraction\n",
        "        val = np.random.rand() * 100\n",
        "        status = \"warning\"\n",
        "        note = \"placeholder_extraction\"\n",
        "\n",
        "        all_rows.append({\n",
        "            \"source_image\": img_path, \"indicator\": indicator, \"date\": date_iso,\n",
        "            \"series_name\": \"series_1\", \"value\": val, \"extraction_status\": status, \"note\": note,\n",
        "        })\n",
        "\n",
        "    if all_rows:\n",
        "        out_path = os.path.join(paths[\"extracted\"], f\"{indicator}_extracted.csv\")\n",
        "        pd.DataFrame(all_rows).to_csv(out_path, index=False)\n",
        "        extracted_files.append(out_path)\n",
        "print(\"Extraction complete.\")\n",
        "\n",
        "# %%\n",
        "# 7) Merge extracted → master (normalisasi tanggal)\n",
        "# (Tidak ada perubahan di sel ini)\n",
        "files = sorted(glob.glob(os.path.join(paths[\"extracted\"], \"*_extracted.csv\")))\n",
        "if not files:\n",
        "    master = pd.DataFrame(columns=[\"indicator\", \"date\", \"value\"])\n",
        "else:\n",
        "    dfs = [pd.read_csv(f) for f in files]\n",
        "    master = pd.concat(dfs, ignore_index=True)\n",
        "    master['date'] = pd.to_datetime(master['date'], errors='coerce').dt.strftime('%Y-%m-%d')\n",
        "\n",
        "master_path = os.path.join(paths[\"output\"], \"extracted_master.csv\")\n",
        "master.to_csv(master_path, index=False)\n",
        "print(\"Master extracted written:\", master_path)\n",
        "\n",
        "# %%\n",
        "# 8) Impute (pivot → iterative imputer) — menulis master_imputed.csv\n",
        "# (Tidak ada perubahan di sel ini, imputasi kompleks tetap di CPU)\n",
        "df = pd.read_csv(master_path)\n",
        "if df.empty:\n",
        "    pd.DataFrame().to_csv(os.path.join(paths[\"output\"], \"master_imputed.csv\"))\n",
        "    print(\"Master empty, skipping imputation.\")\n",
        "else:\n",
        "    df_agg = df.groupby([\"indicator\", \"date\"])['value'].mean().reset_index()\n",
        "    pivot = df_agg.pivot(index=\"date\", columns=\"indicator\", values=\"value\").sort_index()\n",
        "    for c in pivot.columns:\n",
        "        pivot[f\"{c}_missing_flag\"] = pivot[c].isna().astype(int)\n",
        "\n",
        "    num_cols = [c for c in pivot.columns if not c.endswith(\"_missing_flag\")]\n",
        "    if not pivot.empty and len(num_cols) > 0:\n",
        "        imputer = IterativeImputer(random_state=42, max_iter=10)\n",
        "        pivot[num_cols] = imputer.fit_transform(pivot[num_cols])\n",
        "\n",
        "    out_imputed = os.path.join(paths[\"output\"], \"master_imputed.csv\")\n",
        "    pivot.to_csv(out_imputed)\n",
        "    print(\"Imputed master written:\", out_imputed)\n",
        "\n",
        "\n",
        "# %%\n",
        "# 9) Feature engineering (lags, rolling, temporal)\n",
        "imputed_path = os.path.join(paths[\"output\"], \"master_imputed.csv\")\n",
        "if not os.path.exists(imputed_path):\n",
        "    print(\"No imputed file available — skipping feature engineering.\")\n",
        "else:\n",
        "    # --- MODIFIKASI: Pilih antara cuDF (GPU) atau Pandas (CPU) ---\n",
        "    if USE_CUDA:\n",
        "        print(\"Running Feature Engineering on GPU with cuDF...\")\n",
        "        df = cudf.read_csv(imputed_path)\n",
        "        df['date'] = cudf.to_datetime(df['date'], errors='coerce')\n",
        "        df = df.dropna(subset=['date']).sort_values('date')\n",
        "\n",
        "        feat = df.copy()\n",
        "        feat['year'] = feat['date'].dt.year\n",
        "        feat['month'] = feat['date'].dt.month\n",
        "        feat['quarter'] = feat['date'].dt.quarter\n",
        "\n",
        "        value_cols = [c for c in feat.columns if c not in ['date', 'year', 'month', 'quarter']]\n",
        "        lags = [1, 3, 6]\n",
        "        for c in value_cols:\n",
        "            for L in lags:\n",
        "                feat[f\"{c}_lag_{L}\"] = feat[c].shift(L)\n",
        "            feat[f\"{c}_rolling_mean_3\"] = feat[c].rolling(3).mean()\n",
        "            feat[f\"{c}_rolling_std_3\"] = feat[c].rolling(3).std()\n",
        "\n",
        "        # cuDF tidak punya pct_change, kita hitung manual\n",
        "        for c in value_cols:\n",
        "             for p in [1, 3]:\n",
        "                shifted = feat[c].shift(p)\n",
        "                feat[f'{c}_pct_change_{p}'] = (feat[c] - shifted) / shifted\n",
        "\n",
        "        # Tulis ke file dari GPU\n",
        "        features_path = os.path.join(paths[\"output\"], \"features_engineered.csv\")\n",
        "        feat.to_csv(features_path, index=False)\n",
        "        print(\"Features engineered (GPU) written:\", features_path)\n",
        "\n",
        "    else:\n",
        "        print(\"Running Feature Engineering on CPU with Pandas...\")\n",
        "        df = pd.read_csv(imputed_path)\n",
        "        df['date'] = pd.to_datetime(df['date'], errors='coerce')\n",
        "        df = df.dropna(subset=['date']).sort_values('date')\n",
        "\n",
        "        feat = df.copy()\n",
        "        feat['year'] = feat['date'].dt.year\n",
        "        feat['month'] = feat['date'].dt.month\n",
        "        feat['quarter'] = feat['date'].dt.quarter\n",
        "\n",
        "        value_cols = [c for c in feat.columns if c not in ['date', 'year', 'month', 'quarter']]\n",
        "        lags = [1, 3, 6]\n",
        "        for c in value_cols:\n",
        "            for L in lags:\n",
        "                feat[f\"{c}_lag_{L}\"] = feat[c].shift(L)\n",
        "            feat[f\"{c}_rolling_mean_3\"] = feat[c].rolling(3).mean()\n",
        "            feat[f\"{c}_rolling_std_3\"] = feat[c].rolling(3).std()\n",
        "            feat[f\"{c}_pct_change_1\"] = feat[c].pct_change(1)\n",
        "            feat[f\"{c}_pct_change_3\"] = feat[c].pct_change(3)\n",
        "\n",
        "        features_path = os.path.join(paths[\"output\"], \"features_engineered.csv\")\n",
        "        feat.to_csv(features_path, index=False)\n",
        "        print(\"Features engineered (CPU) written:\", features_path)\n",
        "\n",
        "# %%\n",
        "# 10) Load training targets & align with features\n",
        "train_files = sorted(glob.glob(os.path.join(DATA_DIR, \"C*\", \"train*.csv\")))\n",
        "\n",
        "if not train_files:\n",
        "    print(\"No train*.csv found. Cannot train.\")\n",
        "else:\n",
        "    # --- MODIFIKASI: Pilih antara cuDF (GPU) atau Pandas (CPU) ---\n",
        "    if USE_CUDA:\n",
        "        print(\"Loading and aligning data on GPU with cuDF...\")\n",
        "        trains = [cudf.read_csv(p) for p in train_files]\n",
        "        train_df = cudf.concat(trains, ignore_index=True)\n",
        "        feat = cudf.read_csv(os.path.join(paths[\"output\"], \"features_engineered.csv\"))\n",
        "    else:\n",
        "        print(\"Loading and aligning data on CPU with Pandas...\")\n",
        "        trains = [pd.read_csv(p) for p in train_files]\n",
        "        train_df = pd.concat(trains, ignore_index=True)\n",
        "        feat = pd.read_csv(os.path.join(paths[\"output\"], \"features_engineered.csv\"))\n",
        "\n",
        "    if LABEL_COL in train_df.columns:\n",
        "        # Peta label tetap di CPU karena sederhana, lalu pindahkan ke GPU jika perlu\n",
        "        mapped_labels = train_df[LABEL_COL].to_pandas().map(LABEL_MAP).values\n",
        "        if USE_CUDA:\n",
        "            train_df[LABEL_COL] = cudf.Series(mapped_labels)\n",
        "        else:\n",
        "            train_df[LABEL_COL] = mapped_labels\n",
        "    else:\n",
        "        raise KeyError(f\"Label column '{LABEL_COL}' not found.\")\n",
        "\n",
        "    common_key = 'date'\n",
        "    if common_key not in feat.columns or common_key not in train_df.columns:\n",
        "         print(f\"No common key '{common_key}' found. Cannot merge.\")\n",
        "    else:\n",
        "        # Melakukan merge\n",
        "        if USE_CUDA:\n",
        "             merged = cudf.merge(train_df, feat, on=common_key, how='inner')\n",
        "        else:\n",
        "             merged = pd.merge(train_df, feat, on=common_key, how='inner')\n",
        "\n",
        "        if merged.empty:\n",
        "            print(\"Merging features with training yielded empty set.\")\n",
        "        else:\n",
        "            y = merged[LABEL_COL]\n",
        "            X = merged.drop(columns=[LABEL_COL])\n",
        "            print(\"Aligned X, y shapes:\", X.shape, y.shape)\n",
        "\n",
        "            # %%\n",
        "            # 11) Train & evaluate\n",
        "            if USE_CUDA:\n",
        "                print(\"Training and evaluating on GPU with cuML...\")\n",
        "                num_cols = X.select_dtypes(include=['float64', 'int64']).columns.tolist()\n",
        "\n",
        "                # Preprocessing manual untuk cuML (tanpa pipeline)\n",
        "                imputer = cuml_SimpleImputer(strategy=\"median\")\n",
        "                scaler = cuml_StandardScaler(with_mean=False)\n",
        "                X_num_imputed = imputer.fit_transform(X[num_cols])\n",
        "                X_processed = scaler.fit_transform(X_num_imputed)\n",
        "\n",
        "                candidates = {\n",
        "                    \"logreg\": cuml_LogisticRegression(class_weight='balanced'),\n",
        "                    \"rf\": cuml_RandomForestClassifier(n_estimators=300, max_depth=16, random_state=42)\n",
        "                }\n",
        "\n",
        "                best_score = -1.0\n",
        "                best_name = None\n",
        "                splitter = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "                for name, clf in candidates.items():\n",
        "                    cv_scores = []\n",
        "                    for train_idx, val_idx in splitter.split(X_processed.get(), y.get()): # .get() untuk konversi ke numpy\n",
        "                        X_tr, X_va = X_processed[train_idx], X_processed[val_idx]\n",
        "                        y_tr, y_va = y.iloc[train_idx], y.iloc[val_idx]\n",
        "\n",
        "                        clf.fit(X_tr, y_tr)\n",
        "                        preds = clf.predict(X_va)\n",
        "                        cv_scores.append(cuml_f1_score(y_va, preds, average='macro'))\n",
        "\n",
        "                    score = float(cp.mean(cp.array(cv_scores)))\n",
        "                    if score > best_score:\n",
        "                        best_score = score\n",
        "                        best_name = name\n",
        "\n",
        "                print(f\"Best model on GPU: {best_name} with CV F1-score: {best_score:.4f}\")\n",
        "\n",
        "                # Latih model terbaik pada data penuh dan simpan\n",
        "                best_model = candidates[best_name]\n",
        "                best_model.fit(X_processed, y)\n",
        "                model_path = os.path.join(paths[\"models\"], \"best_model.pkl\")\n",
        "                with open(model_path, \"wb\") as f:\n",
        "                    pickle.dump(best_model, f)\n",
        "                print(\"Best model (cuML) saved:\", model_path)\n",
        "\n",
        "                # Evaluasi holdout\n",
        "                X_tr, X_te, y_tr, y_te = cuml_train_test_split(X_processed, y, test_size=0.2, random_state=42, stratify=y)\n",
        "                best_model.fit(X_tr, y_tr)\n",
        "                pred = best_model.predict(X_te)\n",
        "                macro = cuml_f1_score(y_te, pred, average='macro')\n",
        "                cm = cuml_confusion_matrix(y_te, pred)\n",
        "                # Classification report tidak ada di cuML, kita buat dari scikit-learn\n",
        "                cls_rep = classification_report(y_te.get(), pred.get(), digits=4)\n",
        "\n",
        "            else:\n",
        "                print(\"Training and evaluating on CPU with Scikit-learn...\")\n",
        "                num_cols = X.select_dtypes(include=[np.number]).columns.tolist()\n",
        "                cat_cols = X.select_dtypes(exclude=[np.number]).columns.tolist()\n",
        "\n",
        "                pre = ColumnTransformer(transformers=[\n",
        "                    (\"num\", Pipeline([(\"imputer\", SimpleImputer(strategy=\"median\")), (\"scaler\", StandardScaler(with_mean=False))]), num_cols),\n",
        "                    (\"cat\", Pipeline([(\"imputer\", SimpleImputer(strategy=\"most_frequent\"))]), cat_cols)\n",
        "                ])\n",
        "\n",
        "                candidates = {\n",
        "                    \"logreg\": LogisticRegression(max_iter=200, class_weight='balanced'),\n",
        "                    \"rf\": RandomForestClassifier(n_estimators=300, max_depth=None, random_state=42, class_weight='balanced')\n",
        "                }\n",
        "\n",
        "                best_pipe = None\n",
        "                best_score = -1.0\n",
        "                best_name = None\n",
        "                splitter = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "                for name, clf in candidates.items():\n",
        "                    pipe = Pipeline([(\"pre\", pre), (\"clf\", clf)])\n",
        "                    cv_scores = []\n",
        "                    for train_idx, val_idx in splitter.split(X, y):\n",
        "                        X_tr, X_va = X.iloc[train_idx], X.iloc[val_idx]\n",
        "                        y_tr, y_va = y.iloc[train_idx], y.iloc[val_idx]\n",
        "                        pipe.fit(X_tr, y_tr)\n",
        "                        preds = pipe.predict(X_va)\n",
        "                        cv_scores.append(f1_score(y_va, preds, average='macro'))\n",
        "                    score = np.mean(cv_scores)\n",
        "                    if score > best_score:\n",
        "                        best_score, best_name, best_pipe = score, name, pipe\n",
        "\n",
        "                print(f\"Best model on CPU: {best_name} with CV F1-score: {best_score:.4f}\")\n",
        "\n",
        "                best_pipe.fit(X, y)\n",
        "                model_path = os.path.join(paths[\"models\"], \"best_model.joblib\")\n",
        "                dump(best_pipe, model_path)\n",
        "                print(\"Best model (scikit-learn) saved:\", model_path)\n",
        "\n",
        "                X_tr, X_te, y_tr, y_te = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "                best_pipe.fit(X_tr, y_tr)\n",
        "                pred = best_pipe.predict(X_te)\n",
        "                macro = f1_score(y_te, pred, average='macro')\n",
        "                cls_rep = classification_report(y_te, pred, digits=4)\n",
        "                cm = confusion_matrix(y_te, pred)\n",
        "\n",
        "            # Tulis laporan (hasil dikonversi ke format CPU jika perlu)\n",
        "            report_cm = cm.get() if USE_CUDA else cm\n",
        "            report_macro = float(macro) if not USE_CUDA else macro.item()\n",
        "            report_best_name = best_name\n",
        "\n",
        "            report = textwrap.dedent(f\"\"\"\n",
        "            EVALUATION REPORT (Auto-generated on {'GPU' if USE_CUDA else 'CPU'})\n",
        "            ====================================================\n",
        "            Best Model: {report_best_name}\n",
        "            CV Macro F1 (mean): {best_score:.4f}\n",
        "            Holdout Macro F1: {report_macro:.4f}\n",
        "            Per-class metrics:\n",
        "            {cls_rep}\n",
        "            Confusion matrix:\n",
        "            {report_cm}\n",
        "            \"\"\").strip()\n",
        "            with open(os.path.join(paths[\"reports\"], \"evaluation_report.txt\"), \"w\", encoding=\"utf-8\") as f:\n",
        "                f.write(report)\n",
        "            print(\"Evaluation report written.\")\n",
        "\n",
        "            # %%\n",
        "            # 12) Build submission\n",
        "            test_files = sorted(glob.glob(os.path.join(DATA_DIR, \"C*\", \"test*.csv\")))\n",
        "            if not test_files:\n",
        "                print(\"No test*.csv found. Skipping prediction.\")\n",
        "            else:\n",
        "                if USE_CUDA:\n",
        "                    print(\"Building submission on GPU...\")\n",
        "                    # Muat model cuML, imputer, dan scaler\n",
        "                    with open(model_path, 'rb') as f: model = pickle.load(f)\n",
        "\n",
        "                    tests = [cudf.read_csv(p) for p in test_files]\n",
        "                    test_df = cudf.concat(tests, ignore_index=True)\n",
        "                    merged_test = cudf.merge(test_df, feat, on=common_key, how='left')\n",
        "\n",
        "                    X_test_num = merged_test[num_cols]\n",
        "                    X_test_imputed = imputer.transform(X_test_num)\n",
        "                    X_test_processed = scaler.transform(X_test_imputed)\n",
        "\n",
        "                    preds = model.predict(X_test_processed)\n",
        "\n",
        "                    sub = merged_test[[\"id\"]].copy() if \"id\" in merged_test.columns else cudf.DataFrame()\n",
        "                    sub[\"label\"] = preds\n",
        "                    # Konversi ke pandas sebelum menyimpan\n",
        "                    sub = sub.to_pandas()\n",
        "\n",
        "                else:\n",
        "                    print(\"Building submission on CPU...\")\n",
        "                    model = load(model_path)\n",
        "\n",
        "                    tests = [pd.read_csv(p) for p in test_files]\n",
        "                    test_df = pd.concat(tests, ignore_index=True)\n",
        "                    merged_test = pd.merge(test_df, feat, on=common_key, how='left')\n",
        "\n",
        "                    # Scikit-learn pipeline menangani preprocessing secara otomatis\n",
        "                    preds = model.predict(merged_test)\n",
        "\n",
        "                    sub = merged_test[[\"id\"]].copy() if \"id\" in merged_test.columns else pd.DataFrame()\n",
        "                    sub[\"label\"] = preds\n",
        "\n",
        "                out_sub = os.path.join(paths[\"predictions\"], \"test_predictions.csv\")\n",
        "                sub.to_csv(out_sub, index=False)\n",
        "                print(\"Submission written:\", out_sub)\n",
        "\n",
        "# # Catatan Akhir\n",
        "# - Notebook ini sekarang memiliki dua jalur eksekusi: satu untuk CPU dengan `pandas` dan `scikit-learn`, dan satu lagi untuk GPU dengan `cudf` dan `cuml`. Anda dapat mengontrolnya dengan flag `USE_CUDA` di sel pertama.\n",
        "# - Perhatikan bahwa model yang dilatih di GPU (`.pkl`) tidak kompatibel dengan yang dilatih di CPU (`.joblib`), dan sebaliknya.\n",
        "# - Preprocessing untuk cuML dilakukan secara manual (imputer lalu scaler) karena `cuml` tidak memiliki abstraksi `Pipeline` atau `ColumnTransformer` yang setara dengan `scikit-learn`.\n",
        "# - Jika Anda mendapatkan `ImportError` untuk `cudf` atau `cuml`, pastikan Anda telah menginstal RAPIDS dengan benar di environment conda Anda dan menjalankannya dari sana."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
